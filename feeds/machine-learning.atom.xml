<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>bleh - Machine Learning</title><link href="https://iameo.github.io/" rel="alternate"></link><link href="https://iameo.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>https://iameo.github.io/</id><updated>2021-02-03T10:15:00+01:00</updated><entry><title>OOP: concise version</title><link href="https://iameo.github.io/blog/2021/02/03/oop-concise-version/" rel="alternate"></link><published>2021-02-03T10:15:00+01:00</published><updated>2021-02-03T10:15:00+01:00</updated><author><name>Emmanuel Okwudike</name></author><id>tag:iameo.github.io,2021-02-03:/blog/2021/02/03/oop-concise-version/</id><summary type="html">&lt;p&gt;&lt;em&gt;Glimpse of Object Oriented Programming&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;{% notebook ./notebooks/oop.ipynb %}&lt;/p&gt;</content><category term="Machine Learning"></category><category term="OOP"></category><category term="math"></category><category term="python"></category><category term="class"></category><category term="object"></category></entry><entry><title>Simple Linear Regression</title><link href="https://iameo.github.io/blog/2019/02/02/simple-linear-regression/" rel="alternate"></link><published>2019-02-02T01:00:00+01:00</published><updated>2019-02-02T01:00:00+01:00</updated><author><name>Emmanuel Okwudike</name></author><id>tag:iameo.github.io,2019-02-02:/blog/2019/02/02/simple-linear-regression/</id><summary type="html">&lt;p&gt;&lt;em&gt;I blab about Linear Regression, a bit of calculus here and there, gradient descent and made a few plots.&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;A Linear Regression is a statistical approach to modelling the relationship between a scalar response(dependent variable) and one or more explanatory variables(independent variables). It is one of the most basic and studied form of regression analysis and in machine learning, it is one of the fundamental supervised algorithms. A simple Linear Regression would be between a dependent variable and an independent variable.&lt;/p&gt;
&lt;p&gt;In this post I will try explaining some of the concepts of Linear Regression using examples and having a working model in python.&lt;/p&gt;
&lt;h4 id="contents"&gt;Contents:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The concept of Linear Regression.&lt;/li&gt;
&lt;li&gt;Cost Function. A bit of mathematical inference of the Cost Function.&lt;/li&gt;
&lt;li&gt;Gradient Descent! &lt;/li&gt;
&lt;li&gt;Working out an LR example.&lt;/li&gt;
&lt;li&gt;Conclusion and Resources.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="notation"&gt;Notation:&lt;/h4&gt;
&lt;p&gt;The following notations will be used throughout this post, and in cases of minor changes, I shall do well to state an eleborate description as to how and why, otherwise:&lt;/p&gt;
&lt;p&gt;$X$   -   training input(Features)&lt;/p&gt;
&lt;p&gt;$y$  -   training output(Target)&lt;/p&gt;
&lt;p&gt;$(X,y)$   -   A sample of Training set&lt;/p&gt;
&lt;p&gt;$(X^{i}, y^{i})$    -  &lt;em&gt;ith&lt;/em&gt; batch of training set&lt;/p&gt;
&lt;p&gt;$n$   -   Number of training features $(X)$&lt;/p&gt;
&lt;p&gt;$m$  -   Number of training examples&lt;/p&gt;
&lt;p&gt;$\theta_{i}$   -   Parameters of our Hypothesis.&lt;/p&gt;
&lt;h2 id="the-concept-of-linear-regression"&gt;The Concept of Linear Regression.&lt;/h2&gt;
&lt;p&gt;The construct below shows the underlying concept of a Linear regression; given a set of predictors $X$ and an outcome $y$, we fit a straight line that not only explains the data, but also serves as a predictive model for unseen data.&lt;/p&gt;
&lt;p&gt;There are quite a number of assumptions to consider when using linear regression to predict outputs. However, we would be discussing the most common ones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Separability/additivity&lt;/strong&gt;:
It's a very big assumption that plays on the effects of things in isolation adds up when they happen together. In layman's terms, If there exist multiple pieces of information that would affect the outcome of the prediction, each of the available information could be summed up as if they were being used in isolation. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monotonicity/linearity&lt;/strong&gt;:
A monotonic model is one where changing one of the inputs have a corresponding change(up or down) in the response variable. It assumes as the explanatory variables increase so should the outcome, which isn't always true. Thus, monotonicity holds less compared to the linearity of a model, which is merely a restriction on the parameters(regression coefficients), and speaks loudly of the linear relationship between the mean of the response variable and the explanatory variables.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Lack of perfect multicollinearity in the predictors.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cost-function"&gt;Cost Function&lt;/h2&gt;
&lt;p&gt;Put simply, &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Before delving into the cost function, let's shine some light on the &lt;code&gt;Hypothesis&lt;/code&gt;, a parametric responsibe for getting the best function(i.e, the best line) that explains $X$ and $y$&lt;/p&gt;
&lt;p&gt;Hypothesis: $h_{\theta}(x) = \theta_{0} + \theta_{1} X_{1} + \theta_{2} X_{2}+ ... + \theta_{n} X_{n} \label{a}\tag{1}$&lt;/p&gt;
&lt;p&gt;Or, in our case, a SLR: 
$h_{\theta}(x) = \theta_{0} + \theta_{1} x + \epsilon\tag{2}$&lt;/p&gt;
&lt;p&gt;where $\epsilon = \left\lvert y - h_{\theta}(x) \right\rvert$&lt;/p&gt;
&lt;p&gt;$h_{\theta}(x) = \theta_{1} x\tag{3}$&lt;/p&gt;
&lt;p&gt;Taking $\theta_{0}$ to be 0. That is, our &lt;code&gt;intercept&lt;/code&gt; goes through the origin, as seen below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="hypothesis.png" src="/images/hypothesis.png"&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see (a) is the best fit with $\theta_{1}$ given as 1. Here's how:&lt;/p&gt;
&lt;h5 id="calculations"&gt;Calculations:&lt;/h5&gt;
&lt;p&gt;Our training features, in vector notation:  $X \epsilon \mathbb{R}^{4 \times 1}$&lt;/p&gt;
&lt;p&gt;The term $(\frac {1}{2m})$ below is added for convenience, so we can calculate our gradient(slope) in a much interpretable fashion. $m$ being the size of our data,
and in this case, 4.&lt;/p&gt;
&lt;p&gt;Now, let's manually try getting $\theta_{1}$ that minimizes the cost function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(a) Randomly picked, $\theta_{1}$ is 1.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;i.  &lt;strong&gt;Cost Function&lt;/strong&gt;: $J(\theta_{1}) = (\frac{1}{2m}) \sum_{i=1}^{m}\left\lvert(h(x)^{i} - y^{i})\right\rvert^2$&lt;/p&gt;
&lt;p&gt;ii.  $J(\theta_{1}) = (\frac{1}{2m}) \sum_{i=1}^{m}( (\theta_{i} x^{i} - y^{i}) )^2$&lt;/p&gt;
&lt;p&gt;iii.  $J(\theta_{1}) = (\frac{1}{2 \times 4}) ( 0^2 + 0^2 + 0^2 + 0^2 )$, which gives us &lt;em&gt;0&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(b) Randomly picked, $\theta_{1}$ is 0.8.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;i.  $J(\theta_{1}) = (\frac{1}{2 \times 4}) |(0.8 - 1)^2 + (1.5 - 2)^2 + (2.2 - 3)^2 + (2.8 - 4) ^2 |$&lt;/p&gt;
&lt;p&gt;ii. $J(\theta_{1}) = (\frac{1}{2 \times 4}) (2.37)$&lt;/p&gt;
&lt;p&gt;iii. $J(\theta_{1}) = (\frac{2.37}{8})$, which gives us &lt;em&gt;0.296&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Cost Function graph, after trying other arbitary values for $\theta_{1}$: &lt;br&gt;
&lt;img alt="CF.png" src="/images/CostFunction.png"&gt;&lt;/p&gt;
&lt;p&gt;So, what's the value of $\theta_{1}$ that minimizes $J(\theta_{1})$? yeah? ONE! (CF is 0 when $\theta_{1}$ is 1 and 0.296 at a value of 0.8). You can tell just by eyeballing the graph.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In actual practice, you'd be dealing with more than one weight and while you can manually compute your weights(parameters, $\theta$) either by brute-forcing or random-guessing, it's never really a good idea to do so as it can be something of a computational nightmare. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In general, the cost function is often represented as:
$$J(\theta) = (\frac{1}{2}) \sum_{i=1}^{m}\left\lvert(h(x)^{i} - y^{i})\right\rvert^2$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="gradient-descent"&gt;Gradient Descent&lt;/h2&gt;
&lt;p&gt;&lt;img alt="gd1.gif" src="/images/gradient-descent-400.gif"&gt;&lt;/p&gt;
&lt;p&gt;Gradient Descent is an optimization algorithm that aids in finding the right parameters($\theta s$) that minimizes the &lt;strong&gt;cost function&lt;/strong&gt;. Say we have a cost function, $J(\theta_0,\theta_1) = (\frac{1}{m}) \sum_{i=1}^{m} \left| \left(h(x)_i - y_i \right)^2\right| \label{e}$ and we need to minimize it, to get the best fit for our training data, Gradient Descent goes through a routine of finding the "right" $\theta s$ to achieve that.&lt;/p&gt;
&lt;p&gt;Gradient Descent, in non-zorgon: Mary's lamb is at the bottom of a mountain, the &lt;em&gt;minimum&lt;/em&gt;. Mary would very much like to get to her helpless lamb from the top. Mary has a plan. And it is a simple plan; go down the mountain at a steady pace($\alpha$, learning rate), but how would a blindfolded Mary tell what way's down? She came up with the idea of feeling her environment(gradient/slope), until she gets to her lamb at the bottom.&lt;/p&gt;
&lt;p&gt;Mathematically,&lt;/p&gt;
&lt;p&gt;repeat until convergence {
$\theta_{j} := \theta_{j} - \alpha \frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1})$ (for j = 0 and j = 1)
}&lt;/p&gt;
&lt;p&gt;And the update is done as follow, simultaneously:&lt;/p&gt;
&lt;p&gt;temp 0 $:= \theta_{0} - \alpha \frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1})$&lt;br&gt;
temp 1 $:= \theta_{1} - \alpha \frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1})$&lt;/p&gt;
&lt;p&gt;$\theta_{0} := temp0$&lt;br&gt;
$\theta_{1} := temp1$&lt;/p&gt;
&lt;h4 id="concept-of-the-gradient-descent"&gt;Concept of the Gradient Descent:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Start with some $\theta_{0},\theta_{1}$ (e.g., $\theta_{0} = 0, \theta_{1} = 0$)&lt;/li&gt;
&lt;li&gt;Keep updating $\theta_{0},\theta_{1}$ to reduce the cost function $J(\theta_0, \theta_1)$ until we end up at a minimum.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Graphically, &lt;/p&gt;
&lt;p&gt;&lt;img alt="gradient descent gif" src="/images/gradient-descent.gif"&gt;
source: &lt;a href="https://hackernoon.com/dl03-gradient-descent-719aff91c7d6"&gt;hackernoon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Where $b$ and $m$ could be represented as $\theta_{0}$ and $\theta_{1}$ respectively, as in our graph.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT!&lt;/strong&gt;:
Finding the minima(local or global) step-wise isn't completely dependent on $\alpha$ as the gradient(slope) itself plays a major role. The weight(s), from the slope's calculations, goes against the negative gradient and keeps moving until reaching a minima. Also, worthy of note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If the value of $\alpha$ is too small, gradient descent &lt;em&gt;can&lt;/em&gt; be slow.&lt;/li&gt;
&lt;li&gt;If $\alpha$ is too large, gradient descent can overshot the minimum, failing to converge, or even diverge.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="GD_towardsDatascience" src="/images/GD-towardscience.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you are already at a global minimum, $\theta_{n}$ is unchanged, i.e., $$
\begin{array}{c}
\theta_{n} := \theta_{n} - \alpha \times 0\
\theta_{n} := \theta_{n} - 0\
\theta_{n} := \theta_{n}\
\end{array}
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="working-out-an-example"&gt;Working out an example:&lt;/h2&gt;
&lt;p _="%" class="/notebooks/Simple_LR.ipynb" notebook&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4 id="conclusion"&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Linear regression models are a good starting point for regression tasks and while this is an overview of a Simple Linear Regression, do take note that implementations of the regression technique are beyond the scope of what you've seen to this point.&lt;/p&gt;
&lt;p&gt;I had hoped this would contain as much math as I would like but I'm quite ok with the content. In practice it requires a lot of math to &lt;strong&gt;really&lt;/strong&gt; understand the fundamentals.&lt;/p&gt;
&lt;h4 id="resources"&gt;Resources&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Mastering Machine Learning with scikit-learn (by Gavin Hackeling)&lt;/li&gt;
&lt;li&gt;Andrew NG's course on Machine Learning.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Linear_regression"&gt;Wikipedia - Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Machine Learning"></category><category term="ML"></category><category term="math"></category><category term="regression"></category><category term="scikit-learn"></category></entry></feed>
<!DOCTYPE html>
<html lang="en">

<head>

  
  <meta charset="utf-8">


  <title> BLEH | Simple Linear Regression</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/> 
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
  <link href="https://iameo.github.io/" rel="canonical" />


  <!--MATHJAX-->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


  <!-- Feed -->
        <link href="https://iameo.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="bleh Full Atom Feed" />
          <link href="https://iameo.github.io/feeds/{slug}.atom.xml" type="application/atom+xml" rel="alternate" title="bleh Categories Atom Feed" />

  <!-- CSS for fontawesome and animate -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.0/animate.min.css"></script> -->


  <!-- Jupyter css and customized css -->
  <link href="https://iameo.github.io/theme/css/ipynb.css" type="text/css" rel="stylesheet">
  <link href="https://iameo.github.io/theme/css/style.css" type="text/css" rel="stylesheet">



 <!-- Code highlight color scheme -->
      <link href="https://iameo.github.io/theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- CSS OVERRIDE -->

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com/css2?family=Handlee&display=swap" rel="stylesheet">  

  <!-- require -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
  
  <!-- JQuery  --> 
  <script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>

  <!-- plotly -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/plotly.js/1.54.0/plotly.min.js"></script>


  

  <link href="https://iameo.github.io/blog/2019/02/02/simple-linear-regression/" rel="canonical" />

    <meta name="description" content="I blab about Linear Regression, a bit of calculus here and there, gradient descent and made a few plots.">

    <meta name="author" content="Emmanuel Okwudike">

    <meta name="tags" content="ML">
    <meta name="tags" content="math">
    <meta name="tags" content="regression">
    <meta name="tags" content="scikit-learn">




<!-- Open Graph -->
<meta property="og:site_name" content="bleh"/>
<meta property="og:title" content="Simple Linear Regression"/>
<meta property="og:description" content="I blab about Linear Regression, a bit of calculus here and there, gradient descent and made a few plots."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://iameo.github.io/blog/2019/02/02/simple-linear-regression/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-02-02 01:00:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://iameo.github.io/author/emmanuel-okwudike.html">
<meta property="article:section" content="Machine Learning"/>
<meta property="article:tag" content="ML"/>
<meta property="article:tag" content="math"/>
<meta property="article:tag" content="regression"/>
<meta property="article:tag" content="scikit-learn"/>
<meta property="og:image" content="">

<!-- Twitter Card -->
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Simple Linear Regression",
  "headline": "Simple Linear Regression",
  "datePublished": "2019-02-02 01:00:00+01:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Emmanuel Okwudike",
    "url": "https://iameo.github.io/author/emmanuel-okwudike.html"
  },
  "image": "",
  "url": "https://iameo.github.io/blog/2019/02/02/simple-linear-regression/",
  "description": "A Linear Regression is a statistical approach to modelling the relationship between a scalar response(dependent variable) and one or..."
}
</script>
</head>
<!--Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>

              <li role="presentation"><a href="https://iameo.github.io/pages/contact-me.html">Contact Me</a></li>

    </ul>
  </div>
</nav>
    <!-- Progress bar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
        <nav id="navigation">
            <span id="home-button" class="nav-button">
                <a class="home-button" href="https://iameo.github.io/" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>


  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
            <h2 class="article-title">Simple Linear Regression</h2>
    <header>
       
  
        <span class="post-meta">
	    <i class="fa fa-user"></i>
                <a href="https://iameo.github.io/author/emmanuel-okwudike.html">Emmanuel Okwudike</a>
            | <i class="fa fa-calendar"></i> <time datetime="Sat 02 February 2019">Sat 02 February 2019</time>
        </span>

    </header>
	 <p>A Linear Regression is a statistical approach to modelling the relationship between a scalar response(dependent variable) and one or more explanatory variables(independent variables). It is one of the most basic and studied form of regression analysis and in machine learning, it is one of the fundamental supervised algorithms. A simple Linear Regression would be between a dependent variable and an independent variable.</p>
<p>In this post I will try explaining some of the concepts of Linear Regression using examples and having a working model in python.</p>
<h4 id="contents">Contents:</h4>
<ul>
<li>The concept of Linear Regression.</li>
<li>Cost Function. A bit of mathematical inference of the Cost Function.</li>
<li>Gradient Descent! </li>
<li>Working out an LR example.</li>
<li>Conclusion and Resources.</li>
</ul>
<h4 id="notation">Notation:</h4>
<p>The following notations will be used throughout this post, and in cases of minor changes, I shall do well to state an eleborate description as to how and why, otherwise:</p>
<p>$X$   -   training input(Features)</p>
<p>$y$  -   training output(Target)</p>
<p>$(X,y)$   -   A sample of Training set</p>
<p>$(X^{i}, y^{i})$    -  <em>ith</em> batch of training set</p>
<p>$n$   -   Number of training features $(X)$</p>
<p>$m$  -   Number of training examples</p>
<p>$\theta_{i}$   -   Parameters of our Hypothesis.</p>
<h2 id="the-concept-of-linear-regression">The Concept of Linear Regression.</h2>
<p>The construct below shows the underlying concept of a Linear regression; given a set of predictors $X$ and an outcome $y$, we fit a straight line that not only explains the data, but also serves as a predictive model for unseen data.</p>
<p>There are quite a number of assumptions to consider when using linear regression to predict outputs. However, we would be discussing the most common ones:</p>
<ul>
<li>
<p><strong>Separability/additivity</strong>:
It's a very big assumption that plays on the effects of things in isolation adds up when they happen together. In layman's terms, If there exist multiple pieces of information that would affect the outcome of the prediction, each of the available information could be summed up as if they were being used in isolation. </p>
</li>
<li>
<p><strong>Monotonicity/linearity</strong>:
A monotonic model is one where changing one of the inputs have a corresponding change(up or down) in the response variable. It assumes as the explanatory variables increase so should the outcome, which isn't always true. Thus, monotonicity holds less compared to the linearity of a model, which is merely a restriction on the parameters(regression coefficients), and speaks loudly of the linear relationship between the mean of the response variable and the explanatory variables.</p>
</li>
<li>
<p><strong>Lack of perfect multicollinearity in the predictors.</strong></p>
</li>
</ul>
<h2 id="cost-function">Cost Function</h2>
<p>Put simply, </p>
<blockquote>
<p>The cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y.</p>
</blockquote>
<p>Before delving into the cost function, let's shine some light on the <code>Hypothesis</code>, a parametric responsibe for getting the best function(i.e, the best line) that explains $X$ and $y$</p>
<p>Hypothesis: $h_{\theta}(x) = \theta_{0} + \theta_{1} X_{1} + \theta_{2} X_{2}+ ... + \theta_{n} X_{n} \label{a}\tag{1}$</p>
<p>Or, in our case, a SLR: 
$h_{\theta}(x) = \theta_{0} + \theta_{1} x + \epsilon\tag{2}$</p>
<p>where $\epsilon = \left\lvert y - h_{\theta}(x) \right\rvert$</p>
<p>$h_{\theta}(x) = \theta_{1} x\tag{3}$</p>
<p>Taking $\theta_{0}$ to be 0. That is, our <code>intercept</code> goes through the origin, as seen below.</p>
<p><img alt="hypothesis.png" src="/images/hypothesis.png"></p>
<p>We can clearly see (a) is the best fit with $\theta_{1}$ given as 1. Here's how:</p>
<h5 id="calculations">Calculations:</h5>
<p>Our training features, in vector notation:  $X \epsilon \mathbb{R}^{4 \times 1}$</p>
<p>The term $(\frac {1}{2m})$ below is added for convenience, so we can calculate our gradient(slope) in a much interpretable fashion. $m$ being the size of our data,
and in this case, 4.</p>
<p>Now, let's manually try getting $\theta_{1}$ that minimizes the cost function.</p>
<p><strong>(a) Randomly picked, $\theta_{1}$ is 1.</strong></p>
<p>i.  <strong>Cost Function</strong>: $J(\theta_{1}) = (\frac{1}{2m}) \sum_{i=1}^{m}\left\lvert(h(x)^{i} - y^{i})\right\rvert^2$</p>
<p>ii.  $J(\theta_{1}) = (\frac{1}{2m}) \sum_{i=1}^{m}( (\theta_{i} x^{i} - y^{i}) )^2$</p>
<p>iii.  $J(\theta_{1}) = (\frac{1}{2 \times 4}) ( 0^2 + 0^2 + 0^2 + 0^2 )$, which gives us <em>0</em>.</p>
<p><strong>(b) Randomly picked, $\theta_{1}$ is 0.8.</strong></p>
<p>i.  $J(\theta_{1}) = (\frac{1}{2 \times 4}) |(0.8 - 1)^2 + (1.5 - 2)^2 + (2.2 - 3)^2 + (2.8 - 4) ^2 |$</p>
<p>ii. $J(\theta_{1}) = (\frac{1}{2 \times 4}) (2.37)$</p>
<p>iii. $J(\theta_{1}) = (\frac{2.37}{8})$, which gives us <em>0.296</em>.</p>
<p>Cost Function graph, after trying other arbitary values for $\theta_{1}$: <br>
<img alt="CF.png" src="/images/CostFunction.png"></p>
<p>So, what's the value of $\theta_{1}$ that minimizes $J(\theta_{1})$? yeah? ONE! (CF is 0 when $\theta_{1}$ is 1 and 0.296 at a value of 0.8). You can tell just by eyeballing the graph.  </p>
<p><strong>IMPORTANT</strong>:</p>
<ul>
<li>
<p>In actual practice, you'd be dealing with more than one weight and while you can manually compute your weights(parameters, $\theta$) either by brute-forcing or random-guessing, it's never really a good idea to do so as it can be something of a computational nightmare. </p>
</li>
<li>
<p>In general, the cost function is often represented as:
$$J(\theta) = (\frac{1}{2}) \sum_{i=1}^{m}\left\lvert(h(x)^{i} - y^{i})\right\rvert^2$$</p>
</li>
</ul>
<h2 id="gradient-descent">Gradient Descent</h2>
<p><img alt="gd1.gif" src="/images/gradient-descent-400.gif"></p>
<p>Gradient Descent is an optimization algorithm that aids in finding the right parameters($\theta s$) that minimizes the <strong>cost function</strong>. Say we have a cost function, $J(\theta_0,\theta_1) = (\frac{1}{m}) \sum_{i=1}^{m} \left| \left(h(x)_i - y_i \right)^2\right| \label{e}$ and we need to minimize it, to get the best fit for our training data, Gradient Descent goes through a routine of finding the "right" $\theta s$ to achieve that.</p>
<p>Gradient Descent, in non-zorgon: Mary's lamb is at the bottom of a mountain, the <em>minimum</em>. Mary would very much like to get to her helpless lamb from the top. Mary has a plan. And it is a simple plan; go down the mountain at a steady pace($\alpha$, learning rate), but how would a blindfolded Mary tell what way's down? She came up with the idea of feeling her environment(gradient/slope), until she gets to her lamb at the bottom.</p>
<p>Mathematically,</p>
<p>repeat until convergence {
$\theta_{j} := \theta_{j} - \alpha \frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1})$ (for j = 0 and j = 1)
}</p>
<p>And the update is done as follow, simultaneously:</p>
<p>temp 0 $:= \theta_{0} - \alpha \frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1})$<br>
temp 1 $:= \theta_{1} - \alpha \frac{\partial}{\partial \theta_{j}}J(\theta_{0}, \theta_{1})$</p>
<p>$\theta_{0} := temp0$<br>
$\theta_{1} := temp1$</p>
<h4 id="concept-of-the-gradient-descent">Concept of the Gradient Descent:</h4>
<ol>
<li>Start with some $\theta_{0},\theta_{1}$ (e.g., $\theta_{0} = 0, \theta_{1} = 0$)</li>
<li>Keep updating $\theta_{0},\theta_{1}$ to reduce the cost function $J(\theta_0, \theta_1)$ until we end up at a minimum.</li>
</ol>
<p>Graphically, </p>
<p><img alt="gradient descent gif" src="/images/gradient-descent.gif">
source: <a href="https://hackernoon.com/dl03-gradient-descent-719aff91c7d6">hackernoon</a></p>
<p>Where $b$ and $m$ could be represented as $\theta_{0}$ and $\theta_{1}$ respectively, as in our graph.</p>
<p><strong>IMPORTANT!</strong>:
Finding the minima(local or global) step-wise isn't completely dependent on $\alpha$ as the gradient(slope) itself plays a major role. The weight(s), from the slope's calculations, goes against the negative gradient and keeps moving until reaching a minima. Also, worthy of note:</p>
<ul>
<li>If the value of $\alpha$ is too small, gradient descent <em>can</em> be slow.</li>
<li>If $\alpha$ is too large, gradient descent can overshot the minimum, failing to converge, or even diverge.</li>
</ul>
<p><img alt="GD_towardsDatascience" src="/images/GD-towardscience.png"></p>
<ul>
<li>If you are already at a global minimum, $\theta_{n}$ is unchanged, i.e., $$
\begin{array}{c}
\theta_{n} := \theta_{n} - \alpha \times 0\
\theta_{n} := \theta_{n} - 0\
\theta_{n} := \theta_{n}\
\end{array}
$$</li>
</ul>
<h2 id="working-out-an-example">Working out an example:</h2>
<p _="%" class="/notebooks/Simple_LR.ipynb" notebook><br></p>
<h4 id="conclusion">Conclusion</h4>
<p>Linear regression models are a good starting point for regression tasks and while this is an overview of a Simple Linear Regression, do take note that implementations of the regression technique are beyond the scope of what you've seen to this point.</p>
<p>I had hoped this would contain as much math as I would like but I'm quite ok with the content. In practice it requires a lot of math to <strong>really</strong> understand the fundamentals.</p>
<h4 id="resources">Resources</h4>
<ol>
<li>Mastering Machine Learning with scikit-learn (by Gavin Hackeling)</li>
<li>Andrew NG's course on Machine Learning.</li>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression">Wikipedia - Linear Regression</a></li>
</ol>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=Simple Linear Regression&amp;url=https://iameo.github.io/blog/2019/02/02/simple-linear-regression/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="fa fa-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://iameo.github.io/blog/2019/02/02/simple-linear-regression/" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="fa fa-facebook-f"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https://iameo.github.io/blog/2019/02/02/simple-linear-regression/" onclick="window.open(this.href, 'linkedin-share', 'width=490,height=530');return false;">
                        <i class="fa fa-linkedin"></i><span class="hidden">LinkedIn</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="https://iameo.github.io/tag/ml.html">ML</a><a href="https://iameo.github.io/tag/math.html">math</a><a href="https://iameo.github.io/tag/regression.html">regression</a><a href="https://iameo.github.io/tag/scikit-learn.html">scikit-learn</a>                </aside>

                <div class="clear"></div>


                </section>


                <aside class="post-nav">
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer" style="bottom: 0; height: 2.5rem; width: 100%; position: absolute; margin-top: 2rem;">
      <div class="inner">
        <section class="credits">


          <span class="credits-theme">Theme <a href="https://github.com/iameo/atyna" rel="nofollow">atyna</a></span>
          <span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
        </section>
      </div>
    </footer>
  </section>


  <!-- theme js -->
  <script type="text/javascript" src="https://iameo.github.io/theme/js/script.js"></script>

    <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-132444432-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-132444432-1', { 'anonymize_ip': true });
    </script>
    



</body>

</html>